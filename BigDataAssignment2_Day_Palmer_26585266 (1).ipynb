{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "bD07hsna4S68",
    "outputId": "cd9dcedb-859b-47d9-c64f-da9eea12a780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting prettytable\n",
      "  Downloading prettytable-3.12.0-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from prettytable) (0.2.13)\n",
      "Downloading prettytable-3.12.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-3.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\program files\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install seaborn\n",
    "!pip install prettytable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpU3CWleA7O2"
   },
   "source": [
    "Task 1 – PySpark\n",
    "Implementation of Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORwctTPOAP2E"
   },
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EY_tvhwI3t5O",
    "outputId": "612a281c-bbaa-4923-9b23-1daaa864d593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\Student\\Downloads\n",
      "+-------+-------+------+\n",
      "|user_id|item_id|rating|\n",
      "+-------+-------+------+\n",
      "|      1|      1|     2|\n",
      "|      1|      2|     4|\n",
      "|      1|      3|   3.5|\n",
      "|      1|      4|     3|\n",
      "|      1|      5|     4|\n",
      "|      1|      6|   3.5|\n",
      "|      1|      7|   3.5|\n",
      "|      1|      8|     3|\n",
      "|      1|      9|   2.5|\n",
      "|      1|     10|     4|\n",
      "+-------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")\n",
    "\n",
    "#Create a SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"ColabPySpark\").getOrCreate()\n",
    "\n",
    "#Load the dataset\n",
    "relative_path = 'Datasets/Task_1_dataset/ratings.txt'\n",
    "file_path = os.path.join(os.getcwd(), relative_path)\n",
    "df = spark.read.text(file_path)\n",
    "\n",
    "#Rename value as raw_data for clarity\n",
    "df = df.withColumnRenamed(\"value\", \"raw_data\")\n",
    "\n",
    "#Split 'raw_data' into three columns\n",
    "df = df.select(\n",
    "    split(col(\"raw_data\"), \" \").getItem(0).alias(\"user_id\"),\n",
    "    split(col(\"raw_data\"), \" \").getItem(1).alias(\"item_id\"),\n",
    "    split(col(\"raw_data\"), \" \").getItem(2).alias(\"rating\")\n",
    ")\n",
    "\n",
    "#Drop rows with missing values (nulls)\n",
    "df = df.dropna()\n",
    "\n",
    "#Gets rid of any outliers or any invalid values\n",
    "df = df.filter((col(\"user_id\") >= 0) & (col(\"item_id\") >= 0) & (col(\"rating\") <= 4))\n",
    "\n",
    "#Ensure that the 'rating' is numeric value and within range\n",
    "df = df.filter((col(\"rating\").cast(\"float\").isNotNull()) & (col(\"rating\").between(0, 4)))\n",
    "\n",
    "#Show the dataset\n",
    "df.show(10)\n",
    "\n",
    "#Checks if there are any duplicates (says true if there is no duplicates)\n",
    "df.count() == df.dropDuplicates().count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecSu9t4GASZ5"
   },
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m7rzR6RhAN3I",
    "outputId": "dfff4f08-77a6-45ce-8bc6-445b103ec26c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics For Movie Ratings\n",
      "+----------+------------------+\n",
      "|Statistics|     Movie Ratings|\n",
      "+----------+------------------+\n",
      "|     count|             35497|\n",
      "|      mean|3.0028030537791928|\n",
      "|    stddev|0.9186923817926762|\n",
      "|       min|               0.5|\n",
      "|       max|               4.0|\n",
      "+----------+------------------+\n",
      "\n",
      "Top Rated Movies\n",
      "+--------+------------------+\n",
      "|Movie ID|Movie Rating Total|\n",
      "+--------+------------------+\n",
      "|       7|              1044|\n",
      "|      11|               931|\n",
      "|       2|               915|\n",
      "|     207|               883|\n",
      "|       1|               866|\n",
      "|      17|               815|\n",
      "|      13|               807|\n",
      "|     215|               761|\n",
      "|      12|               756|\n",
      "|      10|               750|\n",
      "+--------+------------------+\n",
      "\n",
      "Lowest Rated Movies\n",
      "+--------+------------------+\n",
      "|Movie ID|Movie Rating Total|\n",
      "+--------+------------------+\n",
      "|    1361|                 1|\n",
      "|    1957|                 1|\n",
      "|    1746|                 1|\n",
      "|    1512|                 1|\n",
      "|    1808|                 1|\n",
      "|     853|                 1|\n",
      "|    1870|                 1|\n",
      "|    1903|                 1|\n",
      "|    1897|                 1|\n",
      "|    2034|                 1|\n",
      "+--------+------------------+\n",
      "\n",
      "Top Ten Users Who Rate The Most Movies\n",
      "+-------+-------------------+\n",
      "|user_id|Total Movie Ratings|\n",
      "+-------+-------------------+\n",
      "|    272|                244|\n",
      "|   1187|                232|\n",
      "|    161|                197|\n",
      "|      3|                193|\n",
      "|    969|                174|\n",
      "|     79|                172|\n",
      "|    199|                155|\n",
      "|    702|                147|\n",
      "|    670|                137|\n",
      "|   1039|                129|\n",
      "+-------+-------------------+\n",
      "\n",
      "Number of unique users: 1508\n",
      "Number of unique movies: 2071\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Casts the rating column as a float so it can be used as a numeric value\n",
    "#for the next part\n",
    "df = df.withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "\n",
    "#Convert the Spark DataFrame to a Pandas DataFrame for easier plotting\n",
    "rating_counts_pd = df.toPandas()\n",
    "\n",
    "#Plot the distribution of ratings Vs their value counts\n",
    "sns.displot(rating_counts_pd,\n",
    "            x=\"rating\",\n",
    "            kde=False)\n",
    "\n",
    "#The plotting\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xticks([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0])\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#Analysing the distribution of ratings\n",
    "print(f\"Statistics For Movie Ratings\")\n",
    "stats = df.describe(\"rating\")\n",
    "\n",
    "#Renaming the columns to make them more readable\n",
    "stats_renamed = stats.withColumnRenamed(\"summary\", \"Statistics\") \\\n",
    "                     .withColumnRenamed(\"rating\", \"Movie Ratings\")\n",
    "\n",
    "#Show the renamed statistics\n",
    "stats_renamed.show()\n",
    "\n",
    "#Top rated and lowest rated shows by users calculated by adding up\n",
    " #how many ratings they got\n",
    "top_rated = (df.groupBy(\"item_id\")\n",
    "             .agg(F.count(\"rating\").alias(\"Movie Rating Total\"))\n",
    "             .withColumnRenamed(\"item_id\", \"Movie ID\")\n",
    "             .orderBy(F.desc(\"Movie Rating Total\"))\n",
    "             .limit(10))\n",
    "\n",
    "low_rated = (df.groupBy(\"item_id\")\n",
    "             .agg(F.count(\"rating\").alias(\"Movie Rating Total\"))\n",
    "              .withColumnRenamed(\"item_id\", \"Movie ID\")\n",
    "             .orderBy(\"Movie Rating Total\")\n",
    "             .limit(10))\n",
    "\n",
    "#Show the results for it\n",
    "print(f\"Top Rated Movies\")\n",
    "top_rated.show()\n",
    "print(f\"Lowest Rated Movies\")\n",
    "low_rated.show()\n",
    "\n",
    "#Top 10 users who have rated movies the most\n",
    "print(f\"Top Ten Users Who Rate The Most Movies\")\n",
    "top_users = (df.groupBy(\"user_id\")\n",
    "             .agg(F.count(\"rating\").alias(\"Total Movie Ratings\"))\n",
    "             .orderBy(F.desc(\"Total Movie Ratings\"))\n",
    "             .limit(10))\n",
    "top_users.show()\n",
    "\n",
    "#Display number of unique users and movies\n",
    "unique_users = df.select(\"user_id\").distinct().count()\n",
    "unique_items = df.select(\"item_id\").distinct().count()\n",
    "\n",
    "#Print it to read the amount of users vs movies there are\n",
    "print(f\"Number of unique users: {unique_users}\")\n",
    "print(f\"Number of unique movies: {unique_items}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyT_DRSPnN91"
   },
   "source": [
    "3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBJkH5i18kON"
   },
   "source": [
    "Model based reccomender - ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjXqYoVWnUty",
    "outputId": "4a15c4b4-0a85-40ca-832f-91a8fdc70e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4325489670526855\n",
      "+-------+-----------------------+----------------+\n",
      "|User ID|Movie Recommendation ID|Predicted Rating|\n",
      "+-------+-----------------------+----------------+\n",
      "|1      |1809                   |3.8706324       |\n",
      "|3      |1336                   |4.029119        |\n",
      "|6      |2030                   |3.8857353       |\n",
      "|12     |492                    |4.132997        |\n",
      "|13     |1021                   |5.174495        |\n",
      "|16     |1021                   |3.9443088       |\n",
      "|20     |661                    |4.5062304       |\n",
      "|22     |250                    |4.2408667       |\n",
      "|26     |1398                   |4.1530027       |\n",
      "|27     |134                    |4.119683        |\n",
      "+-------+-----------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Estimator\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#Indexing as integers\n",
    "user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_index\")\n",
    "item_indexer = StringIndexer(inputCol=\"item_id\", outputCol=\"item_index\")\n",
    "\n",
    "#Drop any existing 'user_index' and 'item_index' columns if present so the code doesn't error\n",
    "if \"user_index\" in df.columns and \"item_index\" in df.columns:\n",
    "    df = df.drop(\"user_index\", \"item_index\")\n",
    "\n",
    "#StringIndexers\n",
    "df = user_indexer.fit(df).transform(df)\n",
    "df = item_indexer.fit(df).transform(df)\n",
    "\n",
    "#Convert the ratings into floats\n",
    "df = df.withColumn(\"rating\", df[\"rating\"].cast(\"float\"))\n",
    "\n",
    "#ALS model\n",
    "ALSmodel = ALS(\n",
    "    userCol=\"user_index\",\n",
    "    itemCol=\"item_index\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    implicitPrefs=False,\n",
    "    rank=10,\n",
    "    maxIter=10)\n",
    "\n",
    "#Train the model\n",
    "model = ALSmodel.fit(df)\n",
    "\n",
    "#Generate the movie recommendations for all the users\n",
    "user_recommendations = model.recommendForAllUsers(10)\n",
    "\n",
    "#Make predictions and display\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# Evaluate the model by computing the MAE of the data\n",
    "evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "MAE = evaluator.evaluate(predictions)\n",
    "print(MAE)\n",
    "\n",
    "#Extract recommended movie IDs and the movie predicted ratings\n",
    "user_recommendations = user_recommendations.select(\n",
    "    F.col(\"user_index\").alias(\"User ID\"),\n",
    "    F.col(\"recommendations\").getItem(0).getField(\"item_index\").alias(\"Movie Recommendation ID\"),\n",
    "    F.col(\"recommendations\").getItem(0).getField(\"rating\").alias(\"Predicted Rating\")\n",
    ")\n",
    "\n",
    "#Show the top ten users with their reccomendation for movie\n",
    "#and their predicted movie rating\n",
    "user_recommendations.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMqeq8aanD3q"
   },
   "source": [
    "Task 2 – MapReduce for Margie Travel dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj8KTmc0n1WC"
   },
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMXnTH5mnx63",
    "outputId": "5754b3e6-e305-4035-ecd4-a93828b188e9"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1779.javaToPython.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)\njava.base/java.lang.reflect.Constructor.newInstance(Unknown Source)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Unknown Source)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)\njava.base/java.lang.reflect.Constructor.newInstance(Unknown Source)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Unknown Source)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1654)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\r\n\tat org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:4140)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor134.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#Convert it to RDD\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m flight_data_rdd \u001b[38;5;241m=\u001b[39m \u001b[43mflight_data_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#Extract the departure airport from the dataset and count the flights\u001b[39;00m\n\u001b[0;32m     24\u001b[0m flight_count_rdd \u001b[38;5;241m=\u001b[39m flight_data_rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m row: (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_c2\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\sql\\dataframe.py:214\u001b[0m, in \u001b[0;36mDataFrame.rdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m<class 'pyspark.rdd.RDD'>\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m     jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjavaToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;241m=\u001b[39m RDD(\n\u001b[0;32m    216\u001b[0m         jrdd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_sc, BatchedSerializer(CPickleSerializer())\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1779.javaToPython.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)\njava.base/java.lang.reflect.Constructor.newInstance(Unknown Source)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Unknown Source)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)\njava.base/java.lang.reflect.Constructor.newInstance(Unknown Source)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Unknown Source)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1654)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\r\n\tat org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:4140)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor134.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "os.environ[\"PYSPARK_PYTHON\"] =\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \n",
    "#Load the flight data CSV as DataFrame\n",
    "flight_data_df = spark.read.option(\"header\", \"false\").csv(\"./Datasets/Task_2_dataset/AComp_Passenger_data_no_error.csv\")\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.stop()\n",
    "\n",
    "#increasing power\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"FlightDataAnalysis\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .set(\"spark.driver.memory\",\"4g\") \\\n",
    "    .set(\"spark.network.timeout\",\"800s\") \\\n",
    "    .set(\"spark.executor.heartbeatInterval\",\"100s\") \\\n",
    "    .set(\"spark.python.worker.reuse\",\"false\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession\n",
    "#Convert it to RDD\n",
    "flight_data_rdd = flight_data_df.rdd\n",
    "\n",
    "#Extract the departure airport from the dataset and count the flights\n",
    "flight_count_rdd = flight_data_rdd.map(lambda row: (row['_c2'], 1))\n",
    "\n",
    "#Add up the flight counts by departure airport\n",
    "airport_flight_counts = flight_count_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Create an RDD of just departure airports\n",
    "departure_airports_rdd = airport_flight_counts.map(lambda x: x[0])\n",
    "\n",
    "#Collect and display the flight counts for each airport\n",
    "airport_flight_counts_result = airport_flight_counts.collect()\n",
    "\n",
    "#Find unused airports by subtracting used departure airports from all airports\n",
    "all_airports_rdd = flight_data_rdd.flatMap(lambda row: [row['_c2'], row['_c3']]).distinct()\n",
    "unused_airports_rdd = all_airports_rdd.subtract(departure_airports_rdd)\n",
    "\n",
    "#Collect and display results\n",
    "unused_airports = unused_airports_rdd.collect()\n",
    "\n",
    "#Airports not used for departure\n",
    "print(\"Airports not used as departure points:\")\n",
    "for airport in unused_airports:\n",
    "    print(airport)\n",
    "\n",
    "#Add a space for readability\n",
    "print()\n",
    "\n",
    "#Print final count of flights that happened\n",
    "print(\"Flight count from each airport:\")\n",
    "for airport, count in sorted(airport_flight_counts_result, key=lambda a: a[1], reverse=True):\n",
    "    print(f\"Airport {airport} has {count} flights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXC_jlfcs2ip"
   },
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhffZ2Fxs2QI",
    "outputId": "447081a7-362f-4390-b186-7463589c39ae"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'property' object has no attribute 'option'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 15\u001b[0m\n\u001b[0;32m      6\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf() \\\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlightDataAnalysis\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.heartbeatInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100s\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.python.worker.reuse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#Load the flight data CSV as RDD\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m flight_data_rdd \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Datasets/Task_2_dataset/AComp_Passenger_data_no_error.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrdd\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#Rename columns manually after loading the data\u001b[39;00m\n\u001b[0;32m     18\u001b[0m flight_data_rdd \u001b[38;5;241m=\u001b[39m flight_data_rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m row: (\n\u001b[0;32m     19\u001b[0m     row[\u001b[38;5;241m0\u001b[39m],  \u001b[38;5;66;03m# Passenger_ID\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     row[\u001b[38;5;241m1\u001b[39m],  \u001b[38;5;66;03m# Flight_ID\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     row[\u001b[38;5;241m4\u001b[39m],  \u001b[38;5;66;03m# Dept_Time (timestamp)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m ))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'property' object has no attribute 'option'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#increasing power\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"FlightDataAnalysis\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .set(\"spark.driver.memory\",\"4g\") \\\n",
    "    .set(\"spark.network.timeout\",\"800s\") \\\n",
    "    .set(\"spark.executor.heartbeatInterval\",\"100s\") \\\n",
    "    .set(\"spark.python.worker.reuse\",\"false\")\n",
    "\n",
    "#Load the flight data CSV as RDD\n",
    "flight_data_rdd = spark.read.option(\"header\", \"false\").csv(\"./Datasets/Task_2_dataset/AComp_Passenger_data_no_error.csv\").rdd\n",
    "\n",
    "#Rename columns manually after loading the data\n",
    "flight_data_rdd = flight_data_rdd.map(lambda row: (\n",
    "    row[0],  # Passenger_ID\n",
    "    row[1],  # Flight_ID\n",
    "    row[2],  # Departure_Airport\n",
    "    row[3],  # Arrival_Airport\n",
    "    row[4],  # Dept_Time (timestamp)\n",
    "))\n",
    "\n",
    "#Extract Flight_ID and Passenger_ID for counting passengers\n",
    "flight_passenger_rdd = flight_data_rdd.map(lambda row: (row[1], row[0]))  # (Flight_ID, Passenger_ID)\n",
    "\n",
    "#Count the number of unique passengers per Flight_ID\n",
    "flight_passenger_count_rdd = flight_passenger_rdd.distinct().mapValues(lambda x: 1).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "#Map flight details for further processing\n",
    "flight_info_rdd = flight_data_rdd.map(lambda row: (\n",
    "    row[1],  # Flight_ID\n",
    "    (row[2],  # Departure_Airport\n",
    "     row[3],  # Arrival_Airport\n",
    "     row[4])  # Dept_Time (timestamp)\n",
    "))\n",
    "\n",
    "#Group the flight information with passenger count by Flight_ID\n",
    "joined_rdd = flight_info_rdd.join(flight_passenger_count_rdd)\n",
    "\n",
    "#Convert Unix timestamp to HH:MM format\n",
    "def convert_timestamp(dept_time):\n",
    "  return datetime.utcfromtimestamp(float(dept_time)).strftime('%H:%M')\n",
    "\n",
    "#Map the final result making sure that there is no duplication in the final output\n",
    "flight_summary_rdd = joined_rdd.mapValues(lambda value: (\n",
    "    value[0][0],\n",
    "    value[0][1],\n",
    "    convert_timestamp(value[0][2]),\n",
    "    value[1]\n",
    "))\n",
    "\n",
    "#Collect the results from the RDD into a list\n",
    "flight_summary_list = flight_summary_rdd.map(lambda x: (x[0], x[1])).distinct().collect()\n",
    "\n",
    "#Print a formatted header\n",
    "print(f\"{'Flight ID':<15} {'Departure Airport':<20} {'Arrival Airport':<20} {'Dept Time':<10} {'Total Passengers':<15}\")\n",
    "\n",
    "#Print each of the flight's details\n",
    "for row in flight_summary_list:\n",
    "    flight_id, (departure_airport, arrival_airport, dept_time, total_passengers) = row\n",
    "    print(f\"{flight_id:<15} {departure_airport:<20} {arrival_airport:<20} {dept_time:<10} {total_passengers:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aObnYMr30v0t"
   },
   "source": [
    "3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJ1gAtcc0vLQ",
    "outputId": "93055d93-cafd-4a50-cc26-8657f468387d"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'property' object has no attribute 'option'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 16\u001b[0m\n\u001b[0;32m      7\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf() \\\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlightDataAnalysis\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.heartbeatInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100s\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.python.worker.reuse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#Load the datasets as RDDs\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m flight_data_rdd \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Datasets/Task_2_dataset/AComp_Passenger_data_no_error.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrdd\n\u001b[0;32m     17\u001b[0m airport_data_rdd \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Datasets/Task_2_dataset/Top30_airports_LatLong.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrdd\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#Parse the flight data\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'property' object has no attribute 'option'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from datetime import datetime\n",
    "from prettytable import PrettyTable\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "#increasing power\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"FlightDataAnalysis\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .set(\"spark.driver.memory\",\"4g\") \\\n",
    "    .set(\"spark.network.timeout\",\"800s\") \\\n",
    "    .set(\"spark.executor.heartbeatInterval\",\"100s\") \\\n",
    "    .set(\"spark.python.worker.reuse\",\"false\")\n",
    "\n",
    "#Load the datasets as RDDs\n",
    "flight_data_rdd = spark.read.option(\"header\", \"false\").csv(\"./Datasets/Task_2_dataset/AComp_Passenger_data_no_error.csv\").rdd\n",
    "airport_data_rdd = spark.read.option(\"header\", \"false\").csv(\"./Datasets/Task_2_dataset/Top30_airports_LatLong.csv\").rdd\n",
    "\n",
    "#Parse the flight data\n",
    "flight_data_rdd = flight_data_rdd.map(lambda row: (\n",
    "    row[0],  # Passenger_ID\n",
    "    row[1],  # Flight_ID\n",
    "    row[2],  # Departure_Airport\n",
    "    row[3],  # Arrival_Airport\n",
    "    float(row[4]),  # Dept_Time (timestamp)\n",
    "    int(row[5])  # Flight_Time (mins)\n",
    "))\n",
    "\n",
    "#Parse the airport data\n",
    "airport_data_rdd = airport_data_rdd.map(lambda row: (\n",
    "    row[1],  # IATA/FAA Code\n",
    "    (float(row[2]), float(row[3]))  # Latitude, Longitude\n",
    "))\n",
    "\n",
    "airport_dict = dict(airport_data_rdd.collect())\n",
    "\n",
    "#Function to calculate line-of-sight distance\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 3440.1  # Radius of Earth in nautical miles\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    return R * 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "#Uses MapReduce to calculate the distance\n",
    "flight_distance_rdd = flight_data_rdd.map(lambda row: (\n",
    "    row[0],\n",
    "    row[1],\n",
    "    row[2],\n",
    "    row[3],\n",
    "    datetime.utcfromtimestamp(row[4]).strftime('%H:%M'),\n",
    "    row[5],\n",
    "    calculate_distance(\n",
    "        airport_dict[row[2]][0],\n",
    "        airport_dict[row[2]][1],\n",
    "        airport_dict[row[3]][0],\n",
    "        airport_dict[row[3]][1]\n",
    "    )  # Line-of-sight distance in nautical miles\n",
    "))\n",
    "\n",
    "#Calculate total air miles per passenger\n",
    "passenger_miles_rdd = flight_distance_rdd.map(lambda row: (row[0], row[6]))  # (Passenger_ID, Distance)\n",
    "passenger_miles_rdd = passenger_miles_rdd.reduceByKey(lambda x, y: x + y)  # Sum distances for each Passenger_ID\n",
    "\n",
    "#Find the passenger with the highest air miles (fetch top 1)\n",
    "highest_miles_passenger = passenger_miles_rdd.takeOrdered(1, key=lambda x: -x[1])[0]\n",
    "\n",
    "#PrettyTable to display only the top passenger\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Passenger ID\", \"Total Air Miles\"]\n",
    "\n",
    "#Add only the top passenger's row\n",
    "table.add_row([highest_miles_passenger[0], highest_miles_passenger[1]])\n",
    "\n",
    "#Print the table with the highest air miles passenger\n",
    "print(\"Passenger with the Highest Air Miles:\")\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
